\section{Literature Review}

\subsection{Python 2 vs. 3}
\begin{table*}[b!]
    \begin{tabular}{@{}l|l@{}}
    \toprule
    Python 2             & Python 3              \\ \midrule
    \lstinline[language=Python, style=pythonstyle]|print "Hello World"|  & \lstinline[language=Python, style=pythonstyle]|print("Hello World")|  \\
    \lstinline[language=Python, style=pythonstyle]|exec "print (1 + 1)"| & \lstinline[language=Python, style=pythonstyle]|exec("print (1 + 1)")| \\
    \lstinline[language=Python, style=pythonstyle]|u"Hello World"|       & \lstinline[language=Python, style=pythonstyle]|"Hello World"|\\  \bottomrule      
    \end{tabular}
    \caption{A few examples of changes from Python 2 to 3, from differences in the syntax of statements for printing out text to an output and executing Python code within a Python program, to differences in how text itself is stored differently between both versions.}
    \label{tab:python-2-vs-3}
\end{table*}

The core of the Python language is a program known as the interpreter, which reads Python source code written by a programmer and executes the code line-by-line. A 'version' of the Python language is simply a version of the interpreter, and each new version of the interpreter includes updates that allow the interpreter to read new code syntaxes, as well as other upgrades such as performance boosts. Usually, each new interpreter version is \textit{backwards compatible}, meaning code that executed without issue in the previous version also executes without issue and with the same behavior in the new version \autocite{Malloy}. Backwards compatibility is highly valued due to the high usage of the language, and the PSF usually attempts to maintain backwards compatibility as much as reasonably possible.

However, in December 2008, Python version 3.0 was released. According to the PSF, Python 3.0 was the first ever “intentionally backwards incompatible” release of Python \autocite{vanRossum}. Python 3.0 included numerous syntax changes that meant that the Python 3.0 interpreter was unable to run most code written in Python 2, since it could not recognize the old syntax. According to Guido van Rossum, the creator of Python and head of its development at the time, the purpose of such wide-reaching changes was, “fixing well-known annoyances and warts, and removing a lot of old cruft” \autocite{vanRossum}.

To assist in the massive task of converting entire codebases from Python 2 to Python 3, a program known as 2to3 was created by the PSF \autocite{2to3}. 2to3 is a \textit{transpiler} --- a program that translates one programming language to another --- that converts Python 2 code to Python 3 code. However, 2to3 did not prove as popular as hoped, and the prevailing conversion strategy eventually became to use tools to delicately make a single codebase that could run under Python 2 and Python 3 \autocite{Malloy}. The overall transition from Python 2 to Python 3 has also been rocky, as many Python developers even a decade after the transition began are still attempting to maintain the balance between Python 2 and 3 \autocite{Malloy}. However, all versions of Python 2 have lost support from the PSF, meaning they will not receive any feature updates, or, more critically, any security updates, and all focus is now on maintaining Python 3 \autocite{Sunsetting}. Thus, the importance of converting the remaining Python 2 codebases is great.

\subsection{Code Translation}
The problem of code translation has been approached in a number of different ways. One way has been to use techniques from the field of natural language machine translation. One 2014 study used the technique of statistical machine translation --- translating language based on the probability that a given translation is a correct translation of the original language --- to translate from the C\# programming language to the Java programming language \autocite{Karaivanov}. According to them, 68\% of the code produced by the translator was 'semantically equivalent,' meaning that the translated code was essentially the same as the original code.

Python 2 to 3 translation specifically has not been a widely researched topic within the field of code translation research. However, the one study that has been done on the topic also used the technique of statistical machine translation to translate from Python 2 to Python 3. Researchers from the University of Alberta created the translator and evaluated its effectiveness using a metric called BLEU, which is a metric used by natural language machine translation researchers that is based on the premise that if a given translation is highly similar to known 'correct' translations, then the translation is likely to be correct \autocite{Aggarwal}. Based on this metric, they concluded that their translator very effective. Yet, they also acknowledged that since Python 2 and 3 are still very similar despite their differences, they already achieve high BLEU scores even without translation.

However, a 2020 research paper from Facebook Research (now Meta Research) that used neural machine translation --- translating languages using neural networks --- to translate between various programming languages criticized the use of BLEU as a metric for evaluating the effectiveness of code translators, given that a translation that is missing a 'word' or even a single character may still be highly similar to a correct translation, but still give the wrong result or fail to run at all due to the need for code to be highly precise in a much more extreme way than natural languages do \autocite{Roziere}. They measured the effectiveness of their translator by creating their own metric known as computational accuracy, which is the percentage of translated code that gives the same output when executed as the original code does when executed. Their translator had widely varying success rates depending on the source and target language used. The most successful pairing source and target languages was Java to C++, and the least successful one was Python to Java \autocite{Roziere}.

\subsection{Formal Verification}
Since software of more than trivial complexity started being written, there has been a need for software to be verified to be free of bugs. One of the first models proposed for such verification was Hoare logic, first proposed in 1969 by the British mathematician C.A.R. Hoare. The basic statement in Hoare logic is the Hoare triple, which consists of a command and a precondition and postcondition. If the precondition is met before a command is run, then the command is run, and afterwards the postcondition is met, then the command is considered 'correct' when run with the specified conditions \autocite{Hoare}. Hoare logic has been highly influential, and has spawned the field of formal verification.

Formal verification is nowadays most common in safety-critical industries where verification that the behavior software exhibits matches a specification of what it should do is critical. For example, formal verification is used in the transport industry to ensure that the software running on airplanes and trains exactly matches what engineers believe it should do \autocite{Woodcock}. However, even fields that do not have a critical need for safety have started to adopt formal verification, including the field of compilers. A \textit{compiler} is a type of code translator that translates a programming language that a human programmer can understand into machine code that a computer can recognize and run. Researchers from Inria, a French government-supported computer science research institute, created CompCert, a formally verified compiler for the C programming language \autocite{Leroy}. Compiling programming languages to machine code is a complex task that can result in notoriously difficult to catch bugs that only occur in certain conditions, so the goal of the CompCert project is to verify that all code is compiled correctly with no bugs. CompCert was able to achieve similar performance to mainstream C compilers despite needing more computations than a normal compiler would need due to the need to verify the correctness of compilations, showing that formally verified compilers are feasible to implement. CompCert was so successful that Airbus eventually used CompCert to compile the code that ran on their airplanes, since knowing that the code an engineer writes will be always be correctly translated into machine code is a major component in being sure that the code running on airplanes is safe \autocite{Souyris}.

Research into formal verification of code translators is currently limited outside of CompCert and a project about the formal verification of parts of the LLVM compiler framework from researchers at the University of Pennsylvania \autocite{Leroy}\autocite{Zhao}. There is currently no research about formally verified \textit{transpilers} specifically. This research aims to fill that gap. A Python 2 to Python 3 transpiler was chosen as the research target due to the overall similarity of both versions, despite their large differences, making implementation easier. Such research will hopefully lead to even more research into the application of formally verified transpilers and of formally verified software in general, and hopefully ultimately result in the greater proliferation of formally verified software that can be verified to be free of bugs. The aim of this create experimental research is to answer the questions: to what extent is a formally verified Python 2 to 3 transpiler accurate compared with 2to3, to what extent is a formally verified Python 2 to 3 transpiler faster or slower compared with 2to3, and to what extent is formal verification of a Python 2 to 3 transpiler feasible? The hypothesis for this question was that a formally verified Python 2 to 3 transpiler would be more accurate than 2to3, faster than 2to3, and would be feasible to implement.