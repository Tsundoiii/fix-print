\section{Methodology}

\subsection{Transpiler Creation}
\begin{table*}[b!]
    \begin{tabular}{@{}l|l@{}}
    \toprule
    Python 2             & Python 3              \\ \midrule
    \lstinline[language=Python, style=pythonstyle]|print "Hello World"|  & \lstinline[language=Python, style=pythonstyle]|print("Hello World")|  \\
    \lstinline[language=Python, style=pythonstyle]|print "Hello World",| & \lstinline[language=Python, style=pythonstyle]|print("Hello World", end=' ')| \\
    \lstinline[language=Python, style=pythonstyle]|print >>x, "Hello World"|       & \lstinline[language=Python, style=pythonstyle]|print("Hello World", file=x)|\\  \bottomrule      
    \end{tabular}
    \caption{Specification of how print statements should be translated from Python 2 to 3. For all text inputs to print, they should be surrounded with parentheses in Python 3. If the Python 2 print statement has a trailing comma (indicating that the text input should be outputted without switching to a new line, which is the default behavior), then the Python 3 translation should include \texttt{end=' '} within the parentheses. If the Python 2 print statement has two greater than signs followed by a name before the text input (indicating that the text input should be outputted to a file with that name rather than the screen), then the Python 3 translation should include \texttt{file=<name of file>} within the parentheses.}
    \label{tab:python-print-specification}
\end{table*}

To create a formally verified Python 2 to 3 transpiler, the Coq theorem prover was used, due to its use in previous research into formally verified code translators \autocite{Leroy}\autocite{Zhao}. Coq is a theorem prover, which is a type of software that allows for mathematical theorems to be created and proven using a computer. Coq is a multipurpose theorem prover which originally was used in the development of formal mathematical proofs, but its use has since expanded to general purpose formal verification of software. In addition to mathematical concepts, general-purpose programs can also be written in Coq as well as specifications about what such programs should do \autocite{Coq}. These specifications are treated the exact same as mathematical theorems in the Coq theorem prover, and just as theorem requires several steps before it can be proven, so does a specification about a program. Coq is an interactive theorem prover; when proving either a theorem or a specification in Coq, successive steps (for example, rewrite one side of an equation or evaluate some expression) are issued one-by-one until the theorem or specification is proven. The benefit of an automated theorem prover such as Coq comes from its ability to do tedious computations that can be trusted to be correct, thus reducing the risk of a tiny error ruining a large proof, which has happened many times for theorems which have large proofs.

Originally, the research goal was to create a formally verified transpiler that could perfectly replicate the functionality of 2to3, meaning that the formally verified translator would include every \textit{fixer} --- individual rules in 2to3 that translate one small syntactical change from 2 to 3 --- that 2to3 does. Due to time constraints, that proved infeasible, and instead the research goal was narrowed to only creating a formally verified transpiler for 2to3's \verb|print| fixer. In other words, the formally verified transpiler, dubbed "fix-print," only translated \textit{print statements} --- code statements which print text on a screen and can be used to output text to other outputs --- from Python 2 to 3.

First, a specification, shown in Table \ref{tab:python-print-specification}, for how print statements should be translated was developed based on the Python 2to3 documentation \autocite{2to3}. Then, a \textit{function} --- a specifically defined piece of code that receives an input and returns an output ---  was written in Coq that took in Python 2 print statements and translated them to their Python 3 equivalents. After that, theorems each describing one part of the specification were entered into Coq. For example, one specification theorem was that for all print statements, fix-print's translated code would still begin with the word "print." After each specification theorem was defined, they were then proven in Coq using various strategies.

\subsubsection{Code Extraction}
Due to Coq's nature as a theorem prover rather than a dedicated programming language, executing the transpiler in Coq directly was difficult. To remedy this, Coq's code extraction capabilities were used. Coq's code extraction capabilities allow for programs written in Coq to be translated into fully-fledged programming languages where they can be executed more easily \autocite{Filliatre}. Proofs of program correctness are not extraction in this process, only the programs themselves. However, if a program written in Coq is formally verified to meet specifications, the resulting extracted code should also meet specifications despite not being written in Coq. fix-print was extracted to the OCaml programming language in order to be run, which is also how the CompCert compiler was able to be run despite also being written in Coq \autocite{Leroy}.

\subsection{Evaluation}
fix-print was evaluated against the 2to3 test suite, which is a collection of tests the developers of 2to3 wrote to test the effectiveness 2to3. Specifically, fix-print was tested against the 16 tests that were written to test the effectiveness of the \verb|print| fixer, which are listed in Appendix \ref{appendix:Tests}. For each test, data about fix-print's and 2to3's accuracy and run time performance was collected and stored in CSV files in order to compare the effectiveness of both translators. Accuracy and performance data for each translator is recorded in Appendix \ref{appendix:Data}.

Testing was done on a desktop PC running the Arch Linux operating system with a bash terminal. Each test consisted of one line containing one or more print statements, which were each stored in their own individual files. A helper script found in Appendix \ref{appendix:Helper-Script} written in Python was used to aid collection of accuracy and performance data for both translators.

\subsubsection{Accuracy}
The metric used for evaluating the accuracy of both translators was \textit{computational accuracy}, a metric devised by Roziere et al. in their evaluation of their AI-based code translator \autocite{Roziere}. Translator output code was considered computationally accurate if it outputted the same result as the original input code. Computational accuracy was chosen as the method of evaluation over bilingual evaluation understudy (BLEU), a method of evaluating the accuracy of natural language machine translators that has also been used to evaluate the effectives of machine code translators. BLEU is a measure of the similarity of a translator's output to 'correct' reference translations of the same input, with a higher BLEU score indicating higher similarity to the correct translations. Although BLEU has been used the metric by which past code translation research have evaluated their translators, it is unsuitable as a metric to evaluate code translators due to the fact that small differences between output code and correct code, such as typos or lack of one or two characters, may result in code that produces different output or which does not run at all, yet despite that would still attain high BLEU scores due to its similarity with the correct reference code \autocite{Roziere}. As such, computational accuracy is a preferable metric to measure the effective of a code translator.

To evaluate both translators' computational accuracy, the helper script first ran Python 2 on each test to determine the intended output of each test. Then, the helper script ran 2to3 and fix-print on each test and recorded both the translated code and what the translated code outputted. If the translated code's output was identical to the Python 2 output, the translator was marked as computationally accurate for that test. Both translators' translated code, code outputs, and accuracy for each test are recorded in Appendix \ref{appendix:Accuracy}.

\subsubsection{Performance}
To evaluate both translators' run time performance, the helper script measured how long each translator took to translate each test five times. The mean of those times was then taken to reduce the effect of outliers influencing the run time. To time each translator, the \verb|time| utility of the \verb|bash| command shell which comes preinstalled with each Arch Linux installation was used. All performance testing was conducted on the same machine in the same sitting to reduce outside factors such as how long the machine had been running or what other programs were running on the machine influencing the run time of each translator. Performance data is recorded in Appendix \ref{appendix:Performance}.