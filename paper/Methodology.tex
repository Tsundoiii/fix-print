\section{Methodology}
The following is a description of the methodology of this experimental quantitative study to determine the effectiveness of a formally verified Python 2 to 3 translator. First the software used to create Transpyler is described, followed by a description of Transpyler's creation process, follow by a description of the evaluation method for Transpyler's success, followed by limitations of this study.

\subsection{Software}
\subsubsection{Coq}
Coq is a theorem prover made by Inria, a French government-supported computer science research institute. Coq was chosen due to its use in previous research into formally verified code translators \autocite{Leroy}\autocite{Zhao}.
\subsubsection{Python}
Python 2.7.18 was chosen as the source language for Transpyler due to it being the latest release of Python 2. Python 3.9.17 was chosen as the target language due to Python 3.9 being the latest Python 3 version supported by 2to3 \autocite{2to3}.

\subsection{Creation}
2to3, the most popular Python 2 to 3 translator to date, is composed of a series of \textit{fixers}, which are individual patterns which are matched, then translated to Python 3 \autocite{2to3}.

FIGURE HERE

To create Transpyler, each fixer in 2to3 was defined in Coq. Then, a theorem about said fixer's correctness was proven in Coq. Coq is mainly a theorem prover and although it does model many programming structures quite well, it is still limited in its ability to create programs, as it not primarily a programming language but rather a theorem prover with programming capabilities. Thus Coq's code extraction capabilities, which allow for the generation of programs in various programming languages that conform to theorems verified in Coq, were used to generate code in the OCaml programming language that is verified to be correct. This approach was also used by Leroy et al. in their development of their formally verified compiler using Coq \autocite{Leroy}.

\subsection{Evaluation}
\subsubsection{Metric}
The metric used for evaluating the effectiveness of Transpyler was \textit{computational accuracy}, a metric devised by Roziere et al. in their evaluation of their AI-based code translator \autocite{Roziere}. Translator output code is considered computationally accurate if it outputs the same result as the input. Computational accuracy was chosen as the method of evaluation over bilingual evaluation understudy (BLEU), a method of evaluating the accuracy of natural language machine translators that has also been used to evaluate the effectives of machine code translators. BLEU is a measure of the similarity of a translator's output to 'correct' reference translations of the same input, with a higher BLEU score indicating higher similarity to the correct translations. Although BLEU has been used the metric by which past code translation research have evaluated their translators, it is unsuitable as a metric to evaluate code translators due to the fact that small differences between output code and correct code, such as typos or lack of one or two characters, may result in code that produces different output or which does not run at all, yet despite that would still attain high BLEU scores due to its similarity with the correct reference code \autocite{Roziere}. As such, computational accuracy is a preferable metric to measure the effective of a code translator.
\subsubsection{Data Collection}
The effectiveness of Transpyler was evaluated with a test suite of Python 2 functions. A \textit{function} is a specifically defined piece of code that receives an input and returns an output. Each function was translated by Transpyler, then both the original Python 2 function and the generated Python 3 function were run with a set of inputs. For each input, the generated function's computational accuracy (whether it returned the same output that the original Python 2 code did) was recorded. All data collected was outputted to a file in JSON format.

\subsection{Limitations}
\subsubsection{Validity}
The validility of Transpyler rests on the validity of Coq and its code extraction capabilities. Should Coq or its code extraction capabilities be proven flawed, the validility of Transpyler would also be flawed. However, given Coq's extensive history and use in the fields of both mathematics and computer science, such an outcome is unlikely.
\subsubsection{Scope}
Transpyler does not guarantee valid translations for all Python 2 code, it simply guarantees that any code that matches a fixer pattern will be correctly translated. Any code that does not match a fixer pattern will not be translated, which may result in invalid Python 3 translations.