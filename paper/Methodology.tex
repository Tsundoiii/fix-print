\section{Methodology}

\subsection{Transpiler Creation}
2to3, the most popular Python 2 to 3 translator to date, is composed of a series of \textit{fixers}, which are individual patterns which are matched, then translated to Python 3 \autocite{2to3}.

To create a formally verified Python 2 to 3 transpiler, the Coq theorem prover was used, due to its use in previous research into formally verified code translators \autocite{Leroy}\autocite{Zhao}. Originally, the research goal was to create a formally verified transpiler that could perfectly replicate the functionality of 2to3, meaning that the formally verified translator would include every fixer that 2to3 does. Due to time constraints, that proved infeasible, and instead the research goal was narrowed to only creating a formally verified transpiler for 2to3's \verb|print| fixer. In other words, the formally verified transpiler, dubbed "fix-print," only translated \textit{print statements} -- code statements which print text on a screen and can be used to output text to other outputs -- from Python 2 to 3.

First, a \textit{function} -- a specifically defined piece of code that receives an input and returns an output --  was written in Coq that translated Python 2 print statements to their Python 3 equivalents. Then, theorems each describing one part of the specification were entered into Coq. For example, one specification theorem was that for all print statements, fix-print's translated code would still begin with the word "print." After each specification theorem was defined, they were then proven in Coq using various strategies.

\subsubsection{Code Extraction}
Due to Coq's nature as a theorem prover rather than a dedicated programming language, executing the transpiler in Coq directly was difficult. To remedy this, Coq's code extraction capabilities were used. Coq's code extraction capabilities allow for programs written in Coq to be translated into full-fledged programming languages where they can be executed more easily (https://coq.inria.fr/doc/v8.13/refman/addendum/extraction.html). Proofs of program correctness are not extraction in this process, only the programs themselves. However, if a program written in Coq is formally verified to meet specifications, the resulting extracted code should also meet specifications despite not being written in Coq. fix-print was extracted to the OCaml programming language in order to be run, which is also how the CompCert compiler was able to be run despite also being wirrten in Coq \autocite{Leroy}.

\subsection{Evaluation}
fix-print was evaluated against the 2to3 test suite, which is a collection of tests the developers of 2to3 wrote to test the effectiveness 2to3. Specifically, fix-print was tested against the 16 tests that were written to test the effectiveness of the \verb|print| fixer. For each test, data about fix-print's and 2to3's accuracy and run time performance was collected and stored in CSV files in order to compare the effectiveness of both translators. Accuracy and performance data for each translator is recorded in Appendix \ref{appendix:Data}.

Testing was done on a desktop PC running the Arch Linux operating system with a bash terminal. Each test consisted of one line containing one or more print statements, which were each stored in their own individual files. A helper script found in Appendix [need to link!!] written in Python was used to aid collection of accuracy and performance data for both translators.

\subsubsection{Accuracy}
The metric used for evaluating the accuracy of both translators was \textit{computational accuracy}, a metric devised by Roziere et al. in their evaluation of their AI-based code translator \autocite{Roziere}. Translator output code is considered computationally accurate if it outputs the same result as the input. Computational accuracy was chosen as the method of evaluation over bilingual evaluation understudy (BLEU), a method of evaluating the accuracy of natural language machine translators that has also been used to evaluate the effectives of machine code translators. BLEU is a measure of the similarity of a translator's output to 'correct' reference translations of the same input, with a higher BLEU score indicating higher similarity to the correct translations. Although BLEU has been used the metric by which past code translation research have evaluated their translators, it is unsuitable as a metric to evaluate code translators due to the fact that small differences between output code and correct code, such as typos or lack of one or two characters, may result in code that produces different output or which does not run at all, yet despite that would still attain high BLEU scores due to its similarity with the correct reference code \autocite{Roziere}. As such, computational accuracy is a preferable metric to measure the effective of a code translator.

To evaluate both translators' computational accuracy, the helper script first ran Python 2 on each test to determine the intended output of each test. Then, the helper script ran 2to3 and fix-print on each test and recorded both the translated code and what the translated code outputted. If the translated code's output was identical to the Python 2 output, the translator was marked as computationally accurate for that test. Both translators' translated code, code outputs, and accuracy for each test are recorded in Appendix \ref{appendix:Accuracy}.

\subsubsection{Performance}
To evaluate both translators' run time performance, the helper script measured how long each translator took to translate each test five times. The mean of those times was then taken to reduce the effect of outliers influencing the run time. To time each translator, the \verb|time| utility of the \verb|bash| command shell which comes preinstalled with each Arch Linux installation was used. All performance testing was conducted on the same machine in the same sitting to reduce outside factors such as how long the machine had been running or what other programs were running on the machine influencing the run time of each translator.